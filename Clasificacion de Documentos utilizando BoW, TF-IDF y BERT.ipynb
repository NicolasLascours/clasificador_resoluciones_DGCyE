{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZM8xf-iUc95",
        "outputId": "e99d1176-475d-4b1a-cc05-2fc0fc33061d"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piciBZIqhfwt",
        "outputId": "9c0cbe03-e0bb-40f1-a275-112f555fefb6"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPy_YqtvPAt_",
        "outputId": "c2cd1a9a-3d14-4568-c9f7-bfbfd88ba57b"
      },
      "outputs": [],
      "source": [
        "pip install gensim\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_evXCnUhuiU",
        "outputId": "9e1297ff-2a13-42e1-fdf6-9fb12b3a35ae"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download es_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ugPSbHGKuNe9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRek9j2wuP3L"
      },
      "source": [
        "##LECTURA DE PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ-8fslKZeZU",
        "outputId": "ea04289a-a5db-47a7-c55b-71cc49d9ac30"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "\n",
        "# Ruta del directorio donde están los PDFs\n",
        "ruta_directorio = \"/content/resoluciones/\"\n",
        "\n",
        "# Función para extraer texto de PDFs normales\n",
        "def extraer_texto_pdf(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        with open(ruta_archivo, 'rb') as archivo_pdf:\n",
        "            lector_pdf = PyPDF2.PdfReader(archivo_pdf)\n",
        "            for pagina in range(len(lector_pdf.pages)):\n",
        "                texto += lector_pdf.pages[pagina].extract_text() or \"\"\n",
        "    except PyPDF2.errors.PdfReadError as e:\n",
        "        print(f\"Error al leer el archivo {ruta_archivo}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error al procesar {ruta_archivo}: {e}\")\n",
        "    return texto\n",
        "\n",
        "# Función para extraer texto de PDFs que contienen imágenes usando OCR\n",
        "def extraer_texto_ocr(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        # Convertir las páginas del PDF a imágenes\n",
        "        paginas_imagen = convert_from_path(ruta_archivo)\n",
        "        for pagina_imagen in paginas_imagen:\n",
        "            # Aplicar OCR para extraer el texto de la imagen\n",
        "            texto += pytesseract.image_to_string(pagina_imagen, lang='spa')  # OCR en español\n",
        "    except Exception as e:\n",
        "        print(f\"Error al aplicar OCR a {ruta_archivo}: {e}\")\n",
        "    return texto\n",
        "\n",
        "# Función para procesar PDFs (detectar si tienen texto o son imágenes)\n",
        "def procesar_pdf(ruta_archivo):\n",
        "    try:\n",
        "        texto = extraer_texto_pdf(ruta_archivo)\n",
        "        \n",
        "        if not texto.strip():  # Si no hay texto, aplica OCR\n",
        "            print(f\"Aplicando OCR al archivo {ruta_archivo}\")\n",
        "            texto = extraer_texto_ocr(ruta_archivo)\n",
        "        \n",
        "        return texto\n",
        "    \n",
        "    except Exception as e:\n",
        "        print(f\"Error general al procesar el archivo {ruta_archivo}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Obtener una lista de todos los archivos PDF en el directorio\n",
        "archivos_pdf = [f for f in os.listdir(ruta_directorio) if f.endswith('.pdf')]\n",
        "\n",
        "# Leer y almacenar el contenido de todos los PDFs (texto o OCR)\n",
        "resoluciones = []\n",
        "for archivo in archivos_pdf:\n",
        "    ruta_completa = os.path.join(ruta_directorio, archivo)\n",
        "    texto = procesar_pdf(ruta_completa)\n",
        "    \n",
        "    if texto:  # Solo agregar si se extrajo algún texto\n",
        "        resoluciones.append(texto)\n",
        "\n",
        "# Mostrar el contenido de las primeras resoluciones extraídas\n",
        "for i, texto in enumerate(resoluciones[:3]):\n",
        "    print(f\"Resolución {i + 1}:\\n{texto[:500]}...\\n\")  # Mostramos los primeros 500 caracteres\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5f6OwYPqcW8"
      },
      "source": [
        "##LIMPIEZA DE TEXTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zYamQBag1kB",
        "outputId": "55871a9a-a2e3-4406-9d8e-cdaf08d28efd"
      },
      "outputs": [],
      "source": [
        "# Función para limpiar el texto eliminando números irrelevantes\n",
        "def limpiar_texto(texto):\n",
        "    # 1. Convertir a minúsculas\n",
        "    texto = texto.lower()\n",
        "\n",
        "    # 2. Eliminar números que no sean años relevantes (por ejemplo, '2007')\n",
        "    #    - Mantener los años (números de 4 dígitos que empiezan con 19 o 20)\n",
        "    #    - Eliminar otros números de 1 a 3 dígitos\n",
        "    texto = re.sub(r'\\b(?!19|20)\\d{1,3}\\b', '', texto)  # Conservar años y eliminar otros números\n",
        "\n",
        "    # 3. Eliminar URLs y caracteres especiales (como puntuación), pero conservar palabras y fechas\n",
        "    texto = re.sub(r'[^\\w\\s]https?://\\S+|www\\.\\S+', ' ', texto)\n",
        "\n",
        "    # 4. Eliminar espacios extras (múltiples espacios)\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "\n",
        "    return texto\n",
        "\n",
        "# Aplicar la limpieza\n",
        "resoluciones_limpias = [limpiar_texto(res) for res in resoluciones]\n",
        "\n",
        "# Mostrar el texto limpio (primeros 500 caracteres)\n",
        "print(resoluciones_limpias)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbRwoIlNvXRA"
      },
      "source": [
        "##PREPROCESAMIENTO (Lematización y Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywmQhmCw1LAL",
        "outputId": "1a1790b0-688a-4508-d2c1-c0d80c542280"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo de spaCy para español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Descargar stopwords y recursos de lematización de NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Diccionario de sinónimos para normalizar términos\n",
        "sinonimos = {\n",
        "    \"privados\": \"privado\",\n",
        "    \"privadas\": \"privado\"\n",
        "}\n",
        "\n",
        "# Función para reemplazar sinónimos en el texto\n",
        "def reemplazar_sinonimos(texto):\n",
        "    for key, value in sinonimos.items():\n",
        "        texto = texto.replace(key, value)\n",
        "    return texto\n",
        "\n",
        "# Función para lematizar y eliminar stopwords usando spaCy\n",
        "def lematizar_y_eliminar_stopwords(texto):\n",
        "    doc = nlp(texto)\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
        "    return ' '.join(lemas)\n",
        "\n",
        "# Función para eliminar palabras redundantes\n",
        "def eliminar_palabras_redundantes(texto):\n",
        "    palabras_redundantes = ['de', 'él', 'as', 'ción', 'sr', 'etc', 'co', 'sr']\n",
        "    return ' '.join([palabra for palabra in texto.split() if palabra not in palabras_redundantes])\n",
        "\n",
        "# Función para unificar plurales y singulares (lematización adicional)\n",
        "def lematizar_palabras(vocabulario):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    return ' '.join([lemmatizer.lemmatize(palabra) for palabra in vocabulario.split()])\n",
        "\n",
        "# Aplicar limpieza, lematización, normalización de sinónimos, eliminación de redundancias y unificación de plurales\n",
        "def procesar_resoluciones(res):\n",
        "    # Lematización y eliminación de stopwords con spaCy\n",
        "    res_procesada = lematizar_y_eliminar_stopwords(res)\n",
        "    # Reemplazo de sinónimos\n",
        "    res_procesada = reemplazar_sinonimos(res_procesada)\n",
        "    # Eliminación de palabras redundantes\n",
        "    res_procesada = eliminar_palabras_redundantes(res_procesada)\n",
        "    # Unificación de plurales y singulares\n",
        "    res_procesada = lematizar_palabras(res_procesada)\n",
        "\n",
        "    return res_procesada\n",
        "\n",
        "# Aplicar todo el procesamiento a las resoluciones\n",
        "resoluciones_procesadas = [procesar_resoluciones(res) for res in resoluciones]\n",
        "\n",
        "# Mostrar el resultado de la primera resolución\n",
        "#print(resoluciones_procesadas[0][:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmejI4DgvoA2"
      },
      "source": [
        "##ANÁLISIS con TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XeW6CjnlWvu",
        "outputId": "7d5bfd96-a428-4aef-cb04-81ba9294ac0b"
      },
      "outputs": [],
      "source": [
        "# Inicializar el vectorizador TF-IDF sin eliminación de stopwords (ya fueron eliminadas previamente)\n",
        "vectorizer = TfidfVectorizer(max_features=150, ngram_range=(3,3))\n",
        "\n",
        "# Aplicar TF-IDF a las resoluciones procesadas\n",
        "X = vectorizer.fit_transform(resoluciones_procesadas)\n",
        "\n",
        "# Convertir el resultado a un DataFrame para visualizar las palabras y su peso\n",
        "df_tfidf = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(df_tfidf.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iye_bC35USRK",
        "outputId": "691be681-1754-4e6f-dc6c-d9a57285cc3d"
      },
      "outputs": [],
      "source": [
        "palabras_importantes = df_tfidf.sum().sort_values(ascending=False)\n",
        "print(palabras_importantes.head(20))  # Muestra las 10 palabras más importantes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "otNGyacFUmT1",
        "outputId": "d2ee9586-28ff-4467-c099-ad72d3a0dfdc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "palabras_importantes.head(20).plot(kind='bar')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE4JGYY70yTP"
      },
      "source": [
        "##VISUALIZACIÓN de Resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "fJmHompj0DeK",
        "outputId": "23f3e861-39cf-4097-fa3f-032bccacf04e"
      },
      "outputs": [],
      "source": [
        "# Definir un umbral para las palabras clave\n",
        "umbral = 0.1  # Ajusta según sea necesario\n",
        "\n",
        "# Crear un diccionario para almacenar los temas\n",
        "temas = {}\n",
        "\n",
        "# Agrupar palabras clave por temas\n",
        "for i, row in df_tfidf.iterrows():\n",
        "    palabras_importantes = row[row > umbral].index.tolist()  # Obtener palabras clave con TF-IDF > umbral\n",
        "    for palabra in palabras_importantes:\n",
        "        if palabra not in temas:\n",
        "            temas[palabra] = []\n",
        "        temas[palabra].append(i)  # Agregar el índice de la resolución al tema correspondiente\n",
        "\n",
        "# Crear una nube de palabras para las palabras importantes\n",
        "texto_palabras_importantes = ' '.join(temas.keys())  # Unir las palabras en un solo string\n",
        "\n",
        "# Crear la nube de palabras\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(texto_palabras_importantes)\n",
        "\n",
        "# Visualizar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Nube de Palabras Importantes')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "F_rZAV8oQiKw",
        "outputId": "ce3e6f8d-442b-468f-ed14-9cc05e704f23"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Suponiendo que resoluciones_procesadas es una lista de textos procesados donde cada texto es una lista de palabras\n",
        "resoluciones_tokenizadas = [resolucion.split() for resolucion in resoluciones_procesadas]\n",
        "\n",
        "# Entrenar el modelo Word2Vec\n",
        "modelo_w2v = Word2Vec(sentences=resoluciones_tokenizadas, vector_size=100, window=5, min_count=2, workers=4)\n",
        "\n",
        "# Obtener el vocabulario del modelo\n",
        "palabras_w2v = list(modelo_w2v.wv.index_to_key)\n",
        "\n",
        "# Obtener los embeddings de las palabras\n",
        "X = modelo_w2v.wv[palabras_w2v]\n",
        "\n",
        "# Reducir la dimensionalidad a 2D usando t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=5, n_iter=3000)\n",
        "result_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Crear un DataFrame para usar con Plotly\n",
        "df = pd.DataFrame(result_tsne, columns=['x', 'y'])\n",
        "df['palabra'] = palabras_w2v\n",
        "\n",
        "# Crear gráfico interactivo con Plotly\n",
        "fig = px.scatter(df, x='x', y='y', text='palabra')\n",
        "\n",
        "# Mostrar las palabras al pasar el cursor por encima de los puntos\n",
        "fig.update_traces(textposition='top center', hovertemplate='<b>%{text}</b>')\n",
        "\n",
        "# Mostrar el gráfico interactivo\n",
        "fig.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
