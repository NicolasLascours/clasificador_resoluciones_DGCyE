{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalación de las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_evXCnUhuiU",
        "outputId": "9e1297ff-2a13-42e1-fdf6-9fb12b3a35ae"
      },
      "outputs": [],
      "source": [
        "!pip install spacy\n",
        "!pip install gensim\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalaciones de Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import sqlite3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalaciones de Recursos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRek9j2wuP3L"
      },
      "source": [
        "### Cargamos el contenido de las resoluciones desde la DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ-8fslKZeZU",
        "outputId": "ea04289a-a5db-47a7-c55b-71cc49d9ac30"
      },
      "outputs": [],
      "source": [
        "db_path = \"/content/normativa_dgcye.db\"\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT * FROM archivos ORDER BY id DESC')\n",
        "filas = cursor.fetchall()\n",
        "resoluciones = []\n",
        "# Verificar si se encontró la línea y mostrar el contenido\n",
        "for fila in filas:\n",
        "  resoluciones.append (fila)\n",
        "\n",
        "# Cerrar la conexión a la base de datos\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para limpiar el texto\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = re.sub(r'\\b(?!19|20)\\d{1,3}\\b', '', texto)  # Eliminar números irrelevantes\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', ' ', texto)  # Eliminar URLs\n",
        "    texto = re.sub(r'[^a-zA-Z0-9áéíóúüñ\\s]', '', texto)  # Eliminar caracteres especiales\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()  # Eliminar espacios extras\n",
        "    return texto\n",
        "\n",
        "# Aplicar la limpieza\n",
        "resoluciones_limpias = [limpiar_texto(res[3]) for res in resoluciones]\n",
        "\n",
        "# Mostrar los primeros 1000 caracteres de las resoluciones limpias\n",
        "texto_limpio = ' '.join(resoluciones_limpias)\n",
        "print(texto_limpio[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbRwoIlNvXRA"
      },
      "source": [
        "##PREPROCESAMIENTO (Lematización y Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywmQhmCw1LAL",
        "outputId": "1a1790b0-688a-4508-d2c1-c0d80c542280"
      },
      "outputs": [],
      "source": [
        "# Función para lematizar usando spaCy\n",
        "def lematizar(texto):\n",
        "    doc = nlp(texto)\n",
        "    # Lematizar, eliminando puntuación y espacios\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "    return ' '.join(lemas)\n",
        "\n",
        "for word in [\"dirección\",\"general\", \"de\", \"cultura\", \"y\", \"educación\", \"la\", \"plata\"]:\n",
        "    nlp.vocab[word].is_stop = True\n",
        "\n",
        "# Función para eliminar stopwords usando spaCy\n",
        "def eliminar_stopwords(texto):\n",
        "    doc = nlp(texto)\n",
        "    # Eliminar stopwords, puntuación y espacios\n",
        "    palabras_filtradas = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
        "    return ' '.join(palabras_filtradas)\n",
        "\n",
        "# Aplicar limpieza, lematización y eliminación de stopwords\n",
        "def procesar_resoluciones(res):\n",
        "    res_procesada = lematizar(res)\n",
        "    res_procesada = eliminar_stopwords(res_procesada)\n",
        "    return res_procesada\n",
        "\n",
        "# Procesar todas las resoluciones\n",
        "resoluciones_procesadas = [procesar_resoluciones(res) for res in resoluciones_limpias]\n",
        "\n",
        "# Mostrar el resultado de la primera resolución procesada\n",
        "print(resoluciones_procesadas[0][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis gramatical (POS tagging y NER)\n",
        "Consume muchos recursos y se traba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para realizar POS tagging\n",
        "def etiquetar_partes_discurso(texto):\n",
        "    doc = nlp(texto)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Función para extraer entidades nombradas (NER)\n",
        "def extraer_entidades(texto):\n",
        "    doc = nlp(texto)\n",
        "    entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entidades\n",
        "\n",
        "# Extraer y mostrar POS y NER de las resoluciones procesadas\n",
        "etiquetas = etiquetar_partes_discurso(' '.join(resoluciones_procesadas))\n",
        "print(\"Etiquetas POS:\", etiquetas)\n",
        "\n",
        "entidades_extraidas = [extraer_entidades(res) for res in resoluciones_procesadas]\n",
        "print(\"Entidades extraídas:\", entidades_extraidas[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Salida para evitar el colapso por recursos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Procesar POS y NER para cada resolución individualmente\n",
        "etiquetas = [etiquetar_partes_discurso(res) for res in resoluciones_procesadas]\n",
        "entidades_extraidas = [extraer_entidades(res) for res in resoluciones_procesadas]\n",
        "\n",
        "# Mostrar las primeras etiquetas POS y entidades extraídas\n",
        "print(\"Etiquetas POS de la primera resolución:\", etiquetas[0])\n",
        "print(\"Entidades extraídas de la primera resolución:\", entidades_extraidas[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer().fit(resoluciones_procesadas)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANÁLISIS con BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar el vectorizador BoW\n",
        "vectorizer = CountVectorizer(max_features=1000, ngram_range=(3, 3))  # Ajustar ngram según sea necesario\n",
        "X = vectorizer.fit_transform(resoluciones_procesadas)\n",
        "\n",
        "# Convertir el resultado a un DataFrame\n",
        "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_bow.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficar las 20 palabras más frecuentes\n",
        "palabras_importantes_bow = df_bow.sum().sort_values(ascending=False)\n",
        "print(palabras_importantes_bow.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir un umbral para las palabras clave\n",
        "umbral = 0.3  # Ajustar\n",
        "\n",
        "# Crear un diccionario para almacenar los temas\n",
        "temas = {}\n",
        "\n",
        "# Agrupar palabras clave por temas\n",
        "for i, row in df_bow.iterrows():\n",
        "    palabras_importantes = row[row > umbral].index.tolist()  # Obtener palabras clave con conteo > umbral\n",
        "    for palabra in palabras_importantes:\n",
        "        if palabra not in temas:\n",
        "            temas[palabra] = []  # Inicializa una lista para cada palabra clave\n",
        "        temas[palabra].append(i)  # Agregar el índice de la resolución al tema correspondiente\n",
        "\n",
        "print(\"Palabras clave por tema:\")\n",
        "for indice, palabras in temas.items():\n",
        "    # Convertir cada elemento en 'palabras' a string\n",
        "    palabras_str = [str(palabra) for palabra in palabras]\n",
        "    print(f'Tema: {\", \".join(palabras_str)}; Palabras clave: {indice}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un string para la nube de palabras, asegurando que las claves sean de tipo str\n",
        "nube_texto = ' '.join([str(palabra) for palabra in temas.keys()])  # Unir las palabras clave\n",
        "\n",
        "# Generar la nube de palabras\n",
        "nube_palabras = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(nube_texto)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(nube_palabras, interpolation='bilinear')\n",
        "plt.axis('off')  # No mostrar los ejes\n",
        "plt.title('Nube de Palabras de Palabras Clave por Tema (BoW)', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmejI4DgvoA2"
      },
      "source": [
        "##ANÁLISIS con TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XeW6CjnlWvu",
        "outputId": "7d5bfd96-a428-4aef-cb04-81ba9294ac0b"
      },
      "outputs": [],
      "source": [
        "# Inicializar el vectorizador TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(3,3))\n",
        "X_tfidf = vectorizer.fit_transform(resoluciones_procesadas)\n",
        "\n",
        "# Convertir a DataFrame\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_tfidf.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iye_bC35USRK",
        "outputId": "691be681-1754-4e6f-dc6c-d9a57285cc3d"
      },
      "outputs": [],
      "source": [
        "# Mostrar las palabras más importantes\n",
        "palabras_importantes_tfidf = df_tfidf.sum().sort_values(ascending=False)\n",
        "print(palabras_importantes_tfidf.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "otNGyacFUmT1",
        "outputId": "d2ee9586-28ff-4467-c099-ad72d3a0dfdc"
      },
      "outputs": [],
      "source": [
        "# Definir un umbral para las palabras clave\n",
        "umbral = 0.3  # Ajusta según sea necesario\n",
        "\n",
        "# Crear un diccionario para almacenar las palabras clave por tema\n",
        "temas_tfidf = {}\n",
        "\n",
        "# Agrupar palabras clave por temas\n",
        "for i, row in df_tfidf.iterrows():\n",
        "    palabras_importantes = row[row > umbral].index.tolist()  # Obtener palabras clave con puntuación TF-IDF > umbral\n",
        "    temas_tfidf[i] = palabras_importantes  # Asociar el índice de la resolución con sus palabras clave\n",
        "\n",
        "# Imprimir las palabras por tema\n",
        "print(\"Palabras clave por tema:\")\n",
        "for indice, palabras in temas_tfidf.items():\n",
        "    print(f'Tema {indice}: Palabras clave: {\", \".join(palabras)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un string para la nube de palabras\n",
        "nube_texto_tfidf = ' '.join([palabra for palabras in temas_tfidf.values() for palabra in palabras])  # Unir todas las palabras clave\n",
        "\n",
        "# Generar la nube de palabras\n",
        "nube_palabras_tfidf = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(nube_texto_tfidf)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(nube_palabras_tfidf, interpolation='bilinear')\n",
        "plt.axis('off')  # No mostrar los ejes\n",
        "plt.title('Nube de Palabras de Palabras Clave por Tema (TF-IDF)', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE4JGYY70yTP"
      },
      "source": [
        "### Clustering sobre BoW y TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "fJmHompj0DeK",
        "outputId": "23f3e861-39cf-4097-fa3f-032bccacf04e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Lista para almacenar los coeficientes de silhouette\n",
        "silhouette_scores_bow = []\n",
        "silhouette_scores_tfidf = []\n",
        "\n",
        "# Probar diferentes números de clusters\n",
        "for k in range(2, 11):  # El coeficiente silhouette no se define para k=1\n",
        "    # K-means para BoW\n",
        "    kmeans_bow = KMeans(n_clusters=k, random_state=42)\n",
        "    clusters_bow = kmeans_bow.fit_predict(X)\n",
        "    silhouette_scores_bow.append(silhouette_score(X, clusters_bow))\n",
        "\n",
        "    # K-means para TF-IDF\n",
        "    kmeans_tfidf = KMeans(n_clusters=k, random_state=42)\n",
        "    clusters_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n",
        "    silhouette_scores_tfidf.append(silhouette_score(X_tfidf, clusters_tfidf))\n",
        "\n",
        "# Graficar el coeficiente de silhouette para BoW y TF-IDF\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Silhouette score para BoW\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(2, 11), silhouette_scores_bow, marker='o', color='skyblue')\n",
        "plt.title('Coeficiente Silhouette (BoW)')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "# Silhouette score para TF-IDF\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(2, 11), silhouette_scores_tfidf, marker='o', color='salmon')\n",
        "plt.title('Coeficiente Silhouette (TF-IDF)')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "F_rZAV8oQiKw",
        "outputId": "ce3e6f8d-442b-468f-ed14-9cc05e704f23"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Número de clusters\n",
        "n_clusters = 8  # Ajusta según sea necesario\n",
        "\n",
        "# K-means para Bag of Words\n",
        "kmeans_bow = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_bow = kmeans_bow.fit_predict(X)\n",
        "\n",
        "# K-means para TF-IDF\n",
        "kmeans_tfidf = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar la cantidad de documentos en cada cluster para BoW\n",
        "conteo_clusters_bow = pd.Series(clusters_bow).value_counts().sort_index()\n",
        "\n",
        "# Contar la cantidad de documentos en cada cluster para TF-IDF\n",
        "conteo_clusters_tfidf = pd.Series(clusters_tfidf).value_counts().sort_index()\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico para BoW\n",
        "plt.subplot(1, 2, 1)\n",
        "conteo_clusters_bow.plot(kind='bar', color='skyblue')\n",
        "plt.title('Distribución de Documentos en Clusters (BoW)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Cantidad de Documentos')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Gráfico para TF-IDF\n",
        "plt.subplot(1, 2, 2)\n",
        "conteo_clusters_tfidf.plot(kind='bar', color='salmon')\n",
        "plt.title('Distribución de Documentos en Clusters (TF-IDF)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Cantidad de Documentos')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
