{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkJAY7tPlquo",
        "outputId": "04c455f4-7362-4aa8-a68e-f448522d2d19",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting es-core-news-sm==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.7.0/es_core_news_sm-3.7.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.13.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->es-core-news-sm==3.7.0) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch\n",
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "\n",
        "# Cargar el modelo y el tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "4EFRBRwTlv8V"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cargar DB"
      ],
      "metadata": {
        "id": "RZo-RRarrPHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "db_path = \"/content/normativa_dgcye (1).db\"\n",
        "\n",
        "conn = sqlite3.connect(db_path)\n",
        "cursor = conn.cursor()\n",
        "\n",
        "cursor.execute('SELECT * FROM archivos ORDER BY id DESC')\n",
        "filas = cursor.fetchall()\n",
        "\n",
        "conn.close()\n",
        "\n",
        "# Crear DataFrame\n",
        "column_names = [column[0] for column in cursor.description]\n",
        "resoluciones_df = pd.DataFrame(filas, columns=column_names)"
      ],
      "metadata": {
        "id": "5OuJBjoimBhU"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto = resoluciones_df[\"contenido\"][10]"
      ],
      "metadata": {
        "id": "LyBV9lxhrasp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limpieza del texto"
      ],
      "metadata": {
        "id": "FWLknVK-rVBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, unicodedata\n",
        "\n",
        "def clean_text(texto):\n",
        "    texto = texto.lower()\n",
        "    texto = unicodedata.normalize('NFD', texto)\n",
        "    texto = texto.encode('ascii', 'ignore').decode('utf-8')\n",
        "\n",
        "    # Eliminar números irrelevantes (no completos como años)\n",
        "    #texto = re.sub(r'\\b(19[5-9]\\d|20[0-2]\\d|2030)\\b', '', texto)\n",
        "    # Eliminar URLs\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', ' ', texto)\n",
        "    # Eliminar caracteres especiales\n",
        "    texto = re.sub(r'[^\\w\\s.-]', ' ', texto)\n",
        "    # Eliminar espacios extras\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
        "    return texto\n",
        "\n",
        "def split_text(text, max_tokens=1024):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        current_length += len(word) + 1  # Consideramos un espacio por palabra\n",
        "        if current_length >= max_tokens:\n",
        "            chunks.append(\" \".join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_length = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(\" \".join(current_chunk))\n",
        "    return chunks\n",
        "\n",
        "def summarize_text(text, max_length=130, min_length=30):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    summary_ids = model.generate(inputs, max_length=max_length, min_length=min_length, length_penalty=2.0, num_beams=4)\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    return summary\n",
        "\n",
        "def summarize_large_text(text, max_tokens=1024, max_length=130, min_length=30):\n",
        "    chunks = split_text(text, max_tokens=max_tokens)\n",
        "    summaries = [summarize_text(chunk, max_length=max_length, min_length=min_length) for chunk in chunks]\n",
        "    return \" \".join(summaries)"
      ],
      "metadata": {
        "id": "X6vh3l5hvfVc"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texto_limpio = clean_text(texto)\n",
        "\n",
        "# Resumir el texto limpio\n",
        "resumen = summarize_large_text(\n",
        "    texto_limpio,\n",
        "    max_tokens=1024,\n",
        "    max_length=500,\n",
        "    min_length=200\n",
        ")\n",
        "\n",
        "print(\"Resumen:\")\n",
        "print(resumen)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kc-u24Bfwc2Z",
        "outputId": "000b3acf-49de-41a6-8b32-15143e61c680"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resumen:\n",
            "Un estudiante de la carrera de licenciatura en ciencias de la educacion de la universidad nacional de lujan plantea la situacion originada al trata r de inscribirse en los listados de emergencia para acceso a cargos docentes. No le fue permitido por no acreditar estudios de nivel secundario. Asegura que se le informo que no podia integrar el listado de emergentes y que tampoco podria aspirar a un ingreso com o docente en el sistema educativo provincial. Aplica una resolucion de aplicacion dependientes on las direcciones respectivas que la persona evaluada que ingresa y aprueba the carrera o un porcen taje de ella ha accedido a un nivell superior de conocimientos. Esta resolucion que sera desglosada para su archivo en la direccion de despacho la que en su lugar agregara copia autenticada de la misma comunicar al departamento mesa general de entradas y salidas. Notifica r al consejo general de cultura y educacion a todas las direccione s docentes y a the direccions de tribunales de clasificacion.    ‘’’. ‘”. ”.”’, ””, “”,. ”,”  ,  , . ,.  .  ,. .,  ;. ‚’,.”.,  ”;. “.‚”:  ; ”/”/.’; : ”;’ ’/’%;’#’: ‚;”%;”\";’$’%.’\";”$”+.”%.”.;’);”#”}.’%:”&’%’%,”%”%, %; % %,’&”%:’+”+’£’+.’%-”\",”%-’\", %. %: !’!”! +. .;”|’@’°;‘# $ #“’',’; ’”  ’?’!,’}.”;  :’|”°’.,’€’,'’';’;}’¨;‼’  ;‚,’'’!:’ol’!.’.; ’;.’•’(’)’).’),’undefined’.: ’: ’#, ’undefunded’ : ‘#,  ’%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## StopWords y lemmatización"
      ],
      "metadata": {
        "id": "xM_pOFl7vAJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer ,CountVectorizer\n",
        "import spacy\n",
        "import nltk\n",
        "from spacy.lang.es.stop_words import STOP_WORDS\n",
        "from nltk.corpus import stopwords as stopwords_en\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "byQnGx7K-imT"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjF5HfDgwvOy",
        "outputId": "e804e3cc-e868-44c3-cd7d-ca9a29368a52"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_personalizadas = [\n",
        "    \"ejecutivo\", \"visto\", \"considerando\", \"expediente\", \"dirección\", \"general\", \"de\", \"cultura\", \"y\",\n",
        "    \"educación\", \"la\", \"plata\", \"provincia\", \"buenos\", \"aires\", \"direccion\", \"educacion\", \"establecer\",\n",
        "    \"resolución\", \"resolucion\", \"presente\", \"articulo\", \"of\", \"from\", \"that\", \"notificar\", \"the\"\n",
        "]\n",
        "\n",
        "# Agregar palabras personalizadas a la lista de stopwords de spaCy\n",
        "for word in stopwords_personalizadas:\n",
        "    STOP_WORDS.add(word)\n",
        "\n",
        "stopwords_es = STOP_WORDS\n",
        "stopwords_en = set(stopwords_en.words('english'))\n",
        "stopwords_combinadas = list(stopwords_es.union(stopwords_en))\n",
        "\n",
        "# Verificar que las stopwords personalizadas están incluidas\n",
        "faltantes = [word for word in stopwords_personalizadas if word not in stopwords_combinadas]\n",
        "\n",
        "if not faltantes:\n",
        "    print(\"Todas las palabras personalizadas están incluidas en las stopwords combinadas.\")\n",
        "else:\n",
        "    print(\"Las siguientes palabras no están incluidas en las stopwords combinadas:\", faltantes)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5CjwU6ar8Nm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7377b59-99b4-4e4e-c4bc-cf4d199003dd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Todas las palabras personalizadas están incluidas en las stopwords combinadas.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para lematizar el texto\n",
        "def lemmatize_text(text):\n",
        "    # Procesamos el texto con spaCy\n",
        "    doc = nlp(text)\n",
        "    # Extraemos las lemas (formas base) de las palabras\n",
        "    lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "\n",
        "    return lemmatized_text\n",
        "\n",
        "\n",
        "# Lematizar el texto\n",
        "texto_lemmatizado = lemmatize_text(resumen)\n",
        "\n",
        "print(\"Texto lematizado:\", texto_lemmatizado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tj3oc2LPcX9o",
        "outputId": "9bf36a25-5e9f-4132-88e5-0bf795028fdd"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto lematizado: estudiante carrera licenciatura ciencia universidad nacional lujar plantear situacion originado r inscribir él . peticionante acreditir alumno regular universidad ingresar articul 7 ley 24521 superior . permitir acreditar estudio nivel secundario . ley superior 7 citado permitir universidad reglamente s condición ingreso persona mayorar s 25 ano . consejo and expidiir referir alcance art .7 citado . ingreso carera nivel superior suplir requisito titulo secundario . consejo aprueba sesion fecha 16 -ix-99 dictamen comision asunto legal . directora and resolver 1 .- admitir inscripcion listado correspondiente aspirante docencia . - disponer efecto cumplimentacion establecido precedente as pirante deberar presentar certificacion acreditir ingreso carrera . refrendado vicepresidente 1 consejo . resolver aplicacion caso presente . comunicar departamento mesa entrada salida direccione s docente tribunal clasificacion .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF"
      ],
      "metadata": {
        "id": "BCLsQzkOvL2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_keywords_tfidf(text, num_keywords=10):\n",
        "    # Configurar el vectorizador TF-IDF con las stopwords personalizadas\n",
        "    vectorizer = TfidfVectorizer(max_features=num_keywords, stop_words=stopwords_combinadas)\n",
        "\n",
        "    # Transformar el texto\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "\n",
        "    # Obtener las palabras clave ordenadas por importancia\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scores = tfidf_matrix.toarray().flatten()\n",
        "\n",
        "    # Combinar palabras con sus puntuaciones y ordenarlas\n",
        "    keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n",
        "    return keywords\n",
        "\n",
        "# Llamada a la función con el resumen\n",
        "keywords_tfidf = extract_keywords_tfidf(texto_lemmatizado)\n",
        "print(\"Palabras clave (TF-IDF):\", keywords_tfidf)\n",
        "\n",
        "keywords_df = pd.DataFrame(keywords_tfidf, columns=['Palabra', 'Puntuación'])\n",
        "\n",
        "# Ordenar el DataFrame por puntuación en orden descendente\n",
        "palabras_importantes_tfidf = keywords_df.sort_values(by='Puntuación', ascending=False)\n",
        "\n",
        "# Mostrar las 10 palabras más importantes\n",
        "print(palabras_importantes_tfidf.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYVP9y1lA6U6",
        "outputId": "0ac0d3b8-a0f7-4c2d-b31f-68633c501b6e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave (TF-IDF): [('consejo', 0.3872983346207417), ('ingreso', 0.3872983346207417), ('superior', 0.3872983346207417), ('universidad', 0.3872983346207417), ('acreditir', 0.2581988897471611), ('carrera', 0.2581988897471611), ('citado', 0.2581988897471611), ('ley', 0.2581988897471611), ('nivel', 0.2581988897471611), ('resolver', 0.2581988897471611)]\n",
            "       Palabra  Puntuación\n",
            "0      consejo    0.387298\n",
            "1      ingreso    0.387298\n",
            "2     superior    0.387298\n",
            "3  universidad    0.387298\n",
            "4    acreditir    0.258199\n",
            "5      carrera    0.258199\n",
            "6       citado    0.258199\n",
            "7          ley    0.258199\n",
            "8        nivel    0.258199\n",
            "9     resolver    0.258199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF para bigramas y trigramas"
      ],
      "metadata": {
        "id": "sRpq5hkur_Bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el vectorizador de TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(3, 3), stop_words=stopwords_combinadas)\n",
        "\n",
        "# Transformar el texto\n",
        "X_tfidf = vectorizer.fit_transform([resumen])\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Sumar los valores de las columnas y ordenar\n",
        "palabras_importantes_tfidf = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "print(palabras_importantes_tfidf.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f0fc7GUrc_AX",
        "outputId": "e9c1eba2-e5f6-4b49-edd0-b1f211799514"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 ix 99                              0.101015\n",
            "plantea situacion originada           0.101015\n",
            "peticionante acredita alumno          0.101015\n",
            "personas mayore 25                    0.101015\n",
            "permitido acreditar estudios          0.101015\n",
            "permite universidad reglamente        0.101015\n",
            "originada inscribirse peticionante    0.101015\n",
            "nivel superior suple                  0.101015\n",
            "nivel secundario ley                  0.101015\n",
            "nacional lujan plantea                0.101015\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el vectorizador de TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(2, 2), stop_words=stopwords_combinadas)\n",
        "\n",
        "# Transformar el texto\n",
        "X_tfidf = vectorizer.fit_transform([resumen])\n",
        "\n",
        "# Convertir la matriz dispersa a un DataFrame\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Sumar los valores de las columnas y ordenar\n",
        "palabras_importantes_tfidf = df_tfidf.sum().sort_values(ascending=False)\n",
        "\n",
        "print(palabras_importantes_tfidf.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJja_2NaBHTR",
        "outputId": "d768dcfc-e0b2-4f3c-8db2-976416b9713c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 ix                    0.100504\n",
            "precedente pirante       0.100504\n",
            "pirante debera           0.100504\n",
            "peticionante acredita    0.100504\n",
            "personas mayore          0.100504\n",
            "permitido acreditar      0.100504\n",
            "permite universidad      0.100504\n",
            "originada inscribirse    0.100504\n",
            "nivel superior           0.100504\n",
            "nivel secundario         0.100504\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import unicodedata\n",
        "import re\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# Tokenizer y modelo preentrenado\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def split_text(text, max_tokens=1024):\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
        "\n",
        "def summarize_text(text, max_length=130, min_length=30):\n",
        "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
        "    summary_ids = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        min_length=min_length,\n",
        "        length_penalty=2.0,\n",
        "        num_beams=4\n",
        "    )\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "def summarize_large_text(text, max_tokens=1024, max_length=130, min_length=30):\n",
        "    chunks = split_text(text, max_tokens=max_tokens)\n",
        "    summaries = []\n",
        "\n",
        "    # Paralelización\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        summaries = list(executor.map(lambda chunk: summarize_text(chunk, max_length, min_length), chunks))\n",
        "\n",
        "    return \" \".join(summaries)"
      ],
      "metadata": {
        "id": "LnFRiKEjF2Jd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo spaCy para español\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "# Función para lematizar texto\n",
        "def lemmatize_text(text):\n",
        "    # Procesamos el texto con spaCy\n",
        "    doc = nlp(text)\n",
        "    # Extraemos las lemas (formas base) de las palabras, excluyendo stopwords\n",
        "    lemmatized_text = \" \".join([token.lemma_ for token in doc if not token.is_stop])\n",
        "    return lemmatized_text\n",
        "\n",
        "# Función para extraer palabras clave usando TF-IDF\n",
        "def extract_keywords_tfidf(text, num_keywords=10):\n",
        "    # Configurar el vectorizador TF-IDF con las stopwords personalizadas\n",
        "    vectorizer = TfidfVectorizer(max_features=num_keywords, ngram_range=(1, 1),stop_words=list(stopwords_combinadas))\n",
        "    # Transformar el texto lematizado\n",
        "    tfidf_matrix = vectorizer.fit_transform([text])\n",
        "    # Obtener las palabras clave ordenadas por importancia\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    scores = tfidf_matrix.toarray().flatten()\n",
        "    # Combinar palabras con sus puntuaciones y ordenarlas\n",
        "    keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n",
        "    return keywords\n",
        "\n",
        "# Función integrada que incluye lematización y extracción de palabras clave\n",
        "def process_text_to_keywords(text, num_keywords=10):\n",
        "    # Lematizar el texto\n",
        "    lemmatized_text = lemmatize_text(text)\n",
        "    # Extraer palabras clave con TF-IDF\n",
        "    keywords_tfidf = extract_keywords_tfidf(lemmatized_text, num_keywords=num_keywords)\n",
        "    # Convertir a DataFrame\n",
        "    keywords_df = pd.DataFrame(keywords_tfidf, columns=['Palabra', 'Puntuación'])\n",
        "    # Ordenar el DataFrame por puntuación en orden descendente\n",
        "    return keywords_df.sort_values(by='Puntuación', ascending=False)\n",
        "\n",
        "\n",
        "# Proceso completo\n",
        "palabras_importantes_tfidf = process_text_to_keywords(resumen, num_keywords=10)\n",
        "\n",
        "# Mostrar las palabras clave\n",
        "print(\"Palabras clave (TF-IDF):\")\n",
        "print(palabras_importantes_tfidf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wenfJ5dzd9KF",
        "outputId": "d9560e94-b5c6-4a16-98b0-7a0c10e6f069"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras clave (TF-IDF):\n",
            "        Palabra  Puntuación\n",
            "0       docente    0.612372\n",
            "1       carrera    0.408248\n",
            "2       listado    0.408248\n",
            "3       acceder    0.204124\n",
            "4     inscribir    0.204124\n",
            "5      integrar    0.204124\n",
            "6  licenciatura    0.204124\n",
            "7         lugar    0.204124\n",
            "8         lujar    0.204124\n",
            "9          mesa    0.204124\n"
          ]
        }
      ]
    }
  ]
}