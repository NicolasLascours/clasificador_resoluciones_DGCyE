{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalación de las dependencias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g_evXCnUhuiU",
        "outputId": "9e1297ff-2a13-42e1-fdf6-9fb12b3a35ae"
      },
      "outputs": [],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install pdf2image\n",
        "!pip install pytesseract\n",
        "!pip install spacy\n",
        "!pip install gensim\n",
        "!python -m spacy download es_core_news_sm\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalaciones de Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "import pytesseract\n",
        "import matplotlib.pyplot as plt\n",
        "from textblob import TextBlob\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from pdf2image import convert_from_path\n",
        "from pytesseract import Output\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ugPSbHGKuNe9"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils\n",
        "!apt-get install -y tesseract-ocr tesseract-ocr-spa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Instalaciones de Recursos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nlp = spacy.load(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRek9j2wuP3L"
      },
      "source": [
        "##LECTURA DE PDFs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ-8fslKZeZU",
        "outputId": "ea04289a-a5db-47a7-c55b-71cc49d9ac30"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from pdf2image import convert_from_path\n",
        "import pytesseract\n",
        "from pytesseract import Output\n",
        "\n",
        "# Ruta del directorio donde están los PDFs\n",
        "ruta_directorio = \"/content/resoluciones/\"\n",
        "\n",
        "# Función para extraer texto de PDFs normales\n",
        "def extraer_texto_pdf(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        with open(ruta_archivo, 'rb') as archivo_pdf:\n",
        "            lector_pdf = PyPDF2.PdfReader(archivo_pdf)\n",
        "            for pagina in range(len(lector_pdf.pages)):\n",
        "                texto += lector_pdf.pages[pagina].extract_text() or \"\"\n",
        "    except PyPDF2.errors.PdfReadError as e:\n",
        "        print(f\"Error al leer el archivo {ruta_archivo}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error al procesar {ruta_archivo}: {e}\")\n",
        "    return texto\n",
        "\n",
        "# Función para extraer texto de PDFs que contienen imágenes usando OCR\n",
        "def extraer_texto_ocr(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        # Convertir las páginas del PDF a imágenes\n",
        "        paginas_imagen = convert_from_path(ruta_archivo)\n",
        "        for pagina_imagen in paginas_imagen:\n",
        "            # Aplicar OCR para extraer el texto de la imagen\n",
        "            texto += pytesseract.image_to_string(pagina_imagen, lang='spa')  # OCR en español\n",
        "    except Exception as e:\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5f6OwYPqcW8"
      },
      "source": [
        "##LIMPIEZA DE TEXTO\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zYamQBag1kB",
        "outputId": "55871a9a-a2e3-4406-9d8e-cdaf08d28efd"
      },
      "outputs": [],
      "source": [
        "# Rutas de Poppler y Tesseract\n",
        "ruta_poppler = \"/usr/bin\"\n",
        "pytesseract.pytesseract.tesseract_cmd = \"/usr/bin/tesseract\"  # Configurar Tesseract\n",
        "\n",
        "# Función para extraer texto de PDFs normales\n",
        "def extraer_texto_pdf(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        with open(ruta_archivo, 'rb') as archivo_pdf:\n",
        "            lector_pdf = PyPDF2.PdfReader(archivo_pdf)\n",
        "            for pagina in range(len(lector_pdf.pages)):\n",
        "                texto += lector_pdf.pages[pagina].extract_text() or \"\"\n",
        "    except PyPDF2.errors.PdfReadError as e:\n",
        "        print(f\"Error al leer el archivo {ruta_archivo}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Ocurrió un error al procesar {ruta_archivo}: {e}\")\n",
        "    return texto\n",
        "\n",
        "# Función para extraer texto de PDFs con imágenes usando OCR\n",
        "def extraer_texto_ocr(ruta_archivo):\n",
        "    texto = \"\"\n",
        "    try:\n",
        "        paginas_imagen = convert_from_path(ruta_archivo, poppler_path=ruta_poppler)\n",
        "        for pagina_imagen in paginas_imagen:\n",
        "            texto += pytesseract.image_to_string(pagina_imagen, lang='spa')\n",
        "    except Exception as e:\n",
        "        print(f\"Error al aplicar OCR a {ruta_archivo}: {e}\")\n",
        "    return texto\n",
        "\n",
        "# Función para procesar PDFs (detectar si tienen texto o son imágenes)\n",
        "def procesar_pdf(ruta_archivo):\n",
        "    try:\n",
        "        texto = extraer_texto_pdf(ruta_archivo)\n",
        "        if not texto.strip():\n",
        "            print(f\"Aplicando OCR al archivo {ruta_archivo}\")\n",
        "            texto = extraer_texto_ocr(ruta_archivo)\n",
        "        return texto\n",
        "    except Exception as e:\n",
        "        print(f\"Error general al procesar el archivo {ruta_archivo}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Directorio con los PDFs\n",
        "ruta_directorio = \"/content/resoluciones/\"\n",
        "archivos_pdf = [f for f in os.listdir(ruta_directorio) if f.endswith('.pdf')]\n",
        "\n",
        "# Leer y almacenar el contenido de todos los PDFs\n",
        "resoluciones = []\n",
        "for archivo in archivos_pdf:\n",
        "    ruta_completa = os.path.join(ruta_directorio, archivo)\n",
        "    texto = procesar_pdf(ruta_completa)\n",
        "    if texto:\n",
        "        resoluciones.append(texto)\n",
        "\n",
        "# Mostrar el contenido de las primeras resoluciones extraídas\n",
        "for i, texto in enumerate(resoluciones[:3]):\n",
        "    print(f\"Resolución {i + 1}:\\n{texto[:500]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Limpieza del texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para limpiar el texto\n",
        "def limpiar_texto(texto):\n",
        "    texto = texto.lower()  # Convertir a minúsculas\n",
        "    texto = re.sub(r'\\b(?!19|20)\\d{1,3}\\b', '', texto)  # Eliminar números irrelevantes\n",
        "    texto = re.sub(r'https?://\\S+|www\\.\\S+', ' ', texto)  # Eliminar URLs\n",
        "    texto = re.sub(r'[^a-zA-Z0-9áéíóúüñ\\s]', '', texto)  # Eliminar caracteres especiales\n",
        "    texto = re.sub(r'\\s+', ' ', texto).strip()  # Eliminar espacios extras\n",
        "    return texto\n",
        "\n",
        "# Aplicar la limpieza\n",
        "resoluciones_limpias = [limpiar_texto(res) for res in resoluciones]\n",
        "\n",
        "# Mostrar los primeros 1000 caracteres de las resoluciones limpias\n",
        "texto_limpio = ' '.join(resoluciones_limpias)\n",
        "print(texto_limpio[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbRwoIlNvXRA"
      },
      "source": [
        "##PREPROCESAMIENTO (Lematización y Stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywmQhmCw1LAL",
        "outputId": "1a1790b0-688a-4508-d2c1-c0d80c542280"
      },
      "outputs": [],
      "source": [
        "# Función para lematizar usando spaCy\n",
        "def lematizar(texto):\n",
        "    doc = nlp(texto)\n",
        "    # Lematizar, eliminando puntuación y espacios\n",
        "    lemas = [token.lemma_ for token in doc if not token.is_punct and not token.is_space]\n",
        "    return ' '.join(lemas)\n",
        "\n",
        "for word in [\"dirección\",\"general\", \"de\", \"cultura\", \"y\", \"educación\", \"la\", \"plata\"]:\n",
        "    nlp.vocab[word].is_stop = True\n",
        "\n",
        "# Función para eliminar stopwords usando spaCy\n",
        "def eliminar_stopwords(texto):\n",
        "    doc = nlp(texto)\n",
        "    # Eliminar stopwords, puntuación y espacios\n",
        "    palabras_filtradas = [token.text for token in doc if not token.is_stop and not token.is_punct and not token.is_space]\n",
        "    return ' '.join(palabras_filtradas)\n",
        "\n",
        "# Aplicar limpieza, lematización y eliminación de stopwords\n",
        "def procesar_resoluciones(res):\n",
        "    res_procesada = lematizar(res)\n",
        "    res_procesada = eliminar_stopwords(res_procesada)\n",
        "    return res_procesada\n",
        "\n",
        "# Procesar todas las resoluciones\n",
        "resoluciones_procesadas = [procesar_resoluciones(res) for res in resoluciones_limpias]\n",
        "\n",
        "# Mostrar el resultado de la primera resolución procesada\n",
        "print(resoluciones_procesadas[0][:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Análisis gramatical (POS tagging y NER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Función para realizar POS tagging\n",
        "def etiquetar_partes_discurso(texto):\n",
        "    doc = nlp(texto)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Función para extraer entidades nombradas (NER)\n",
        "def extraer_entidades(texto):\n",
        "    doc = nlp(texto)\n",
        "    entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entidades\n",
        "\n",
        "# Extraer y mostrar POS y NER de las resoluciones procesadas\n",
        "etiquetas = etiquetar_partes_discurso(' '.join(resoluciones_procesadas))\n",
        "print(\"Etiquetas POS:\", etiquetas)\n",
        "\n",
        "entidades_extraidas = [extraer_entidades(res) for res in resoluciones_procesadas]\n",
        "print(\"Entidades extraídas:\", entidades_extraidas[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = CountVectorizer().fit(resoluciones_procesadas)\n",
        "print(\"Tamaño de vocabulario: {}\".format(len(cv.vocabulary_)))\n",
        "print(\"Vocabulario:\\n{}\".format(cv.get_feature_names_out()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ANÁLISIS con BoW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializar el vectorizador BoW\n",
        "vectorizer = CountVectorizer(max_features=1000, ngram_range=(3, 3))  # Ajustar ngram según sea necesario\n",
        "X = vectorizer.fit_transform(resoluciones_procesadas)\n",
        "\n",
        "# Convertir el resultado a un DataFrame\n",
        "df_bow = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_bow.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Graficar las 20 palabras más frecuentes\n",
        "palabras_importantes_bow = df_bow.sum().sort_values(ascending=False)\n",
        "print(palabras_importantes_bow.head(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definir un umbral para las palabras clave\n",
        "umbral = 0.3  # Ajustar\n",
        "\n",
        "# Crear un diccionario para almacenar los temas\n",
        "temas = {}\n",
        "\n",
        "# Agrupar palabras clave por temas\n",
        "for i, row in df_bow.iterrows():\n",
        "    palabras_importantes = row[row > umbral].index.tolist()  # Obtener palabras clave con conteo > umbral\n",
        "    for palabra in palabras_importantes:\n",
        "        if palabra not in temas:\n",
        "            temas[palabra] = []  # Inicializa una lista para cada palabra clave\n",
        "        temas[palabra].append(i)  # Agregar el índice de la resolución al tema correspondiente\n",
        "\n",
        "print(\"Palabras clave por tema:\")\n",
        "for indice, palabras in temas.items():\n",
        "    # Convertir cada elemento en 'palabras' a string\n",
        "    palabras_str = [str(palabra) for palabra in palabras]\n",
        "    print(f'Tema: {\", \".join(palabras_str)}; Palabras clave: {indice}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un string para la nube de palabras, asegurando que las claves sean de tipo str\n",
        "nube_texto = ' '.join([str(palabra) for palabra in temas.keys()])  # Unir las palabras clave\n",
        "\n",
        "# Generar la nube de palabras\n",
        "nube_palabras = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(nube_texto)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(nube_palabras, interpolation='bilinear')\n",
        "plt.axis('off')  # No mostrar los ejes\n",
        "plt.title('Nube de Palabras de Palabras Clave por Tema (BoW)', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmejI4DgvoA2"
      },
      "source": [
        "##ANÁLISIS con TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XeW6CjnlWvu",
        "outputId": "7d5bfd96-a428-4aef-cb04-81ba9294ac0b"
      },
      "outputs": [],
      "source": [
        "# Inicializar el vectorizador TF-IDF\n",
        "vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(3,3))\n",
        "X_tfidf = vectorizer.fit_transform(resoluciones_procesadas)\n",
        "\n",
        "# Convertir a DataFrame\n",
        "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(df_tfidf.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iye_bC35USRK",
        "outputId": "691be681-1754-4e6f-dc6c-d9a57285cc3d"
      },
      "outputs": [],
      "source": [
        "# Mostrar las palabras más importantes\n",
        "palabras_importantes_tfidf = df_tfidf.sum().sort_values(ascending=False)\n",
        "print(palabras_importantes_tfidf.head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "otNGyacFUmT1",
        "outputId": "d2ee9586-28ff-4467-c099-ad72d3a0dfdc"
      },
      "outputs": [],
      "source": [
        "# Definir un umbral para las palabras clave\n",
        "umbral = 0.3  # Ajusta según sea necesario\n",
        "\n",
        "# Crear un diccionario para almacenar las palabras clave por tema\n",
        "temas_tfidf = {}\n",
        "\n",
        "# Agrupar palabras clave por temas\n",
        "for i, row in df_tfidf.iterrows():\n",
        "    palabras_importantes = row[row > umbral].index.tolist()  # Obtener palabras clave con puntuación TF-IDF > umbral\n",
        "    temas_tfidf[i] = palabras_importantes  # Asociar el índice de la resolución con sus palabras clave\n",
        "\n",
        "# Imprimir las palabras por tema\n",
        "print(\"Palabras clave por tema:\")\n",
        "for indice, palabras in temas_tfidf.items():\n",
        "    print(f'Tema {indice}: Palabras clave: {\", \".join(palabras)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear un string para la nube de palabras\n",
        "nube_texto_tfidf = ' '.join([palabra for palabras in temas_tfidf.values() for palabra in palabras])  # Unir todas las palabras clave\n",
        "\n",
        "# Generar la nube de palabras\n",
        "nube_palabras_tfidf = WordCloud(width=800, height=400, background_color='white', colormap='viridis').generate(nube_texto_tfidf)\n",
        "\n",
        "# Mostrar la nube de palabras\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(nube_palabras_tfidf, interpolation='bilinear')\n",
        "plt.axis('off')  # No mostrar los ejes\n",
        "plt.title('Nube de Palabras de Palabras Clave por Tema (TF-IDF)', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE4JGYY70yTP"
      },
      "source": [
        "### Clustering sobre BoW y TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "fJmHompj0DeK",
        "outputId": "23f3e861-39cf-4097-fa3f-032bccacf04e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Lista para almacenar los coeficientes de silhouette\n",
        "silhouette_scores_bow = []\n",
        "silhouette_scores_tfidf = []\n",
        "\n",
        "# Probar diferentes números de clusters\n",
        "for k in range(2, 11):  # El coeficiente silhouette no se define para k=1\n",
        "    # K-means para BoW\n",
        "    kmeans_bow = KMeans(n_clusters=k, random_state=42)\n",
        "    clusters_bow = kmeans_bow.fit_predict(X)\n",
        "    silhouette_scores_bow.append(silhouette_score(X, clusters_bow))\n",
        "\n",
        "    # K-means para TF-IDF\n",
        "    kmeans_tfidf = KMeans(n_clusters=k, random_state=42)\n",
        "    clusters_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n",
        "    silhouette_scores_tfidf.append(silhouette_score(X_tfidf, clusters_tfidf))\n",
        "\n",
        "# Graficar el coeficiente de silhouette para BoW y TF-IDF\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Silhouette score para BoW\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(2, 11), silhouette_scores_bow, marker='o', color='skyblue')\n",
        "plt.title('Coeficiente Silhouette (BoW)')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "# Silhouette score para TF-IDF\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(2, 11), silhouette_scores_tfidf, marker='o', color='salmon')\n",
        "plt.title('Coeficiente Silhouette (TF-IDF)')\n",
        "plt.xlabel('Número de Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "F_rZAV8oQiKw",
        "outputId": "ce3e6f8d-442b-468f-ed14-9cc05e704f23"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Número de clusters\n",
        "n_clusters = 8  # Ajusta según sea necesario\n",
        "\n",
        "# K-means para Bag of Words\n",
        "kmeans_bow = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_bow = kmeans_bow.fit_predict(X)\n",
        "\n",
        "# K-means para TF-IDF\n",
        "kmeans_tfidf = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "clusters_tfidf = kmeans_tfidf.fit_predict(X_tfidf)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Contar la cantidad de documentos en cada cluster para BoW\n",
        "conteo_clusters_bow = pd.Series(clusters_bow).value_counts().sort_index()\n",
        "\n",
        "# Contar la cantidad de documentos en cada cluster para TF-IDF\n",
        "conteo_clusters_tfidf = pd.Series(clusters_tfidf).value_counts().sort_index()\n",
        "\n",
        "# Graficar los resultados\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Gráfico para BoW\n",
        "plt.subplot(1, 2, 1)\n",
        "conteo_clusters_bow.plot(kind='bar', color='skyblue')\n",
        "plt.title('Distribución de Documentos en Clusters (BoW)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Cantidad de Documentos')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Gráfico para TF-IDF\n",
        "plt.subplot(1, 2, 2)\n",
        "conteo_clusters_tfidf.plot(kind='bar', color='salmon')\n",
        "plt.title('Distribución de Documentos en Clusters (TF-IDF)')\n",
        "plt.xlabel('Cluster')\n",
        "plt.ylabel('Cantidad de Documentos')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
